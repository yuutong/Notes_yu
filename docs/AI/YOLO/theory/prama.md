
## 注意力机制

> 通俗

在信息中找重点，要求对重要的部分放大仔细看，不重要的缩小略过。
注意力机制就是让机器也会“盯”住最关键的信息，对重要部分多“关注”一些，对不重要的部分少“关注”。这样它在处理信息时就不会到处“乱看”，而是把算力用在最该用的地方，效率更高，结果也更准确。<br>

---

> 专业

一种在深度学习神经网络中动态分配权重的技术，突出输入特征序列中对当前任务最有贡献的部分
这个机制包含4个部分：1.Query-Key-Value 架构 2.相关性计算   3. 权重归一化  4.特征聚合

### 1. Query-Key-Value 架构

> 通俗

把输入先 “拆成三份”：<br>
Query（查询）：好比你心里想问的“我要找什么内容？” <br>
Key（关键字）：好比每段话里面的“标签”或“关键字”，告诉你这段话讲的是什么 <br>
Value（内容）：好比每段话的“正文”，真正想要的信息<br>

机器会用三个不同的小“滤镜”把同一份原始信息分别加工成 Q、K、V，方便后面比对和取用

> 专业

* 对于输入特征矩阵 @X\in\mathbb{R}^{n\times d}@（@n@ 个位置，每个是 @d@ 维向量），通过三组可学习的线性变换矩阵 @W_Q,W_K,W_V\in\mathbb{R}^{d\times d_k}@ 或 @\mathbb{R}^{d\times d_v}@，分别生成：

  @@
    Q = XW_Q,\quad K = XW_K,\quad V = XW_V
  @@
* 其中 @Q,K\in\mathbb{R}^{n\times d_k}@，用于后续相似度计算；@V\in\mathbb{R}^{n\times d_v}@，是最终要聚合的信息载体。

---

### 2. 相关性计算（打分）

> 通俗

* 把你想找的“样子”（Q）拿去跟每一段“标签”（K）做对比，看它们有多像：

  * 像的话就给高分，不像就给低分。
* 分数高代表这段信息和你想找的最相关，接下来就优先读它。

> 专业

* 通过点乘（或其它相似度函数）计算 @Q@ 和 @K@ 的相似度矩阵 @S\in\mathbb{R}^{n\times n}@：

  @@
    S = QK^\top
  @@
* 表示第 @i@ 个 Query 与第 @j@ 个 Key 之间的注意力分数（或打分值）。

---

### 3. 权重归一化（Softmax）

> 大白话

* 把刚才打的分数做归一化处理，就像把所有分数加起来变成 100 分制的一部分：

  * 分数高的那几段就瓜分更多“注意力”，分数低的分到的“注意力”就少。
* 这样大家加起来刚好是 1（100%），方便后面分配。

> 专业

* 对每一行（或对每个 Query 的打分向量）做 Softmax 操作，得到注意力权重矩阵 @A@：

  @@
    A_{i,j} = \frac{\exp(S_{i,j})}{\sum_{k=1}^n \exp(S_{i,k})}
  @@
* 每一行 @A_{i,:}@ 都是非负且求和为 1 的分布，表明在第 @i@ 个 Query 视角下，各 Value 的相对重要性。

---

### 4. 特征聚合（加权求和）

> 大白话

* 拿每段“正文”（V），按照刚才分好的“注意力份额”去分配，喜欢的段落多给点注意力、不喜欢的少给点：

  * 把它们加起来，就得到了既包含自己关注点，又融入上下文信息的新“浓缩内容”。

> 专业

* 用注意力权重 @A@ 去加权 Value 矩阵 @V@，得到输出特征矩阵 @O\in\mathbb{R}^{n\times d_v}@：

  @@
    O = A\,V
  @@
* 也就是说，第 @i@ 行 @O_{i,:} = \sum_{j=1}^n A_{i,j}\,V_{j,:}@，它融合了所有位置的 @V@ 特征，按相关性分配权重。

---


### 总结：

1. **生成 Q, K, V**：用三个“滤镜”把输入特征分别转成 Query、Key、Value
2. **计算相似度 (Q·Kᵀ)**：打分，找出哪些 Key 和 Query 最相关
3. **Softmax 归一化**：把分数转成比例权重，和为 1
4. **加权求和 (A·V)**：用权重混合各 Value，得到融合上下文的新特征

整体思路：“先看、再打分、再分配注意力、最后汇总信息”,通过这四步，注意力机制就能“聚焦”到 **真正有用** 的信息片段，使得后续网络层能更精准地处理上下文和细节。
